{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab assignment: SVMs in scikit-learn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this assignment we will learn how to work with SVMs using the scikit-learn library. We will study in detail their cross-validation and pipelining."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. Guidelines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table align=\"center\">\n",
    " <tr><td width=\"80\"><img src=\"img/question.png\" style=\"width:auto;height:auto\"></td><td style=\"text-align:left\">You will need to solve a question by writing your own code or answer in the cell immediately below or in a different file, as instructed.</td></tr>\n",
    " <tr><td width=\"80\"><img src=\"img/exclamation.png\" style=\"width:auto;height:auto\"></td><td style=\"text-align:left\">This is a hint or useful observation that can help you solve a question. You should pay attention to these hints to better understand questions.</td></tr>\n",
    " <tr><td width=\"80\"><img src=\"img/pro.png\" style=\"width:auto;height:auto\"></td><td style=\"text-align:left\">This is an advanced and voluntary exercise that can help you gain a deeper knowledge into the topic. Good luck!</td></tr>\n",
    "</table>\n",
    "\n",
    "During the assignment you will make use of several Python packages that might not be installed in your machine. It is strongly recommended that you use the environment file *environment.yml* and follow the instructions of the following <a href=\"https://github.com/jorloplaz/teaching_material/tree/master/SVM\">link</a>.\n",
    "\n",
    "If you need any help on the usage of a Python function you can place the writing cursor over its name and press Caps+Shift to produce a pop-out with related documentation. This will only work inside code cells.\n",
    "\n",
    "The following code will embed any plots into the notebook instead of generating a new window:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lastly, if you need any help on the usage of a Python function you can place the writing cursor over its name and press Caps+Shift to produce a pop-out with related documentation. This will only work inside code cells.\n",
    "\n",
    "Let's go!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Synthetic dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the first exercises of this assignment we will use the synthetic dataset generated by the following code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.datasets import make_gaussian_quantiles\n",
    "RND_STATE=np.random.RandomState(42)\n",
    "# Build first cluster\n",
    "X1, y1 = make_gaussian_quantiles(cov=2., n_samples=200, n_features=2, n_classes=2, random_state=RND_STATE)\n",
    "# Build second cluster\n",
    "X2, y2 = make_gaussian_quantiles(mean=(3, 3), cov=1.5, n_samples=300, n_features=2, n_classes=2, random_state=RND_STATE)\n",
    "# Fuse them, scaling features differently and switching labels of y2\n",
    "X = np.concatenate((X1, X2))\n",
    "X[:, 0] *= 10\n",
    "X[:, 1] /= 10\n",
    "y = np.concatenate((y1, -y2+1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset consists of 500 points and consists of two Gaussian clusters. In each cluster points in its inner circle belong to one class, and those in the outer circle belong to the other class. Classes are switched in the second cluster so that it is more challenging to discriminate between both classes.\n",
    "\n",
    "If we plot these data we obtain the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib.colors import ListedColormap\n",
    "cm = ListedColormap(['#0000FF', '#FF0000'])    # blue, red\n",
    "plt.scatter(X[:, 0], X[:, 1], c=y, cmap=cm)     \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once we have generated the data (X, y), let us split them into a **training set** and a **test set**. There is a utility function in scikit-learn that does exactly this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.5, random_state=RND_STATE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since *test_size=0.5*, both the training set and the test set will have 250 points each. The number of features is 2, as specified when invoking *make_gaussian_quantiles*:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X_train.shape)\n",
    "print(X_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scikit-learn basics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Scikit-learn's models for **supervised learning** (SVMs are just one kind of such models) implement a common interface. The most important functions of this interface are the following:\n",
    "\n",
    "* **fit(X, y)**: trains the model, fitting it for input patterns X and outputs y.\n",
    "* **score(X, y)**: tests an already fitted model with additional data, returning the accuracy obtained (i.e., how similar the outputs given by the model are, compared to the true outputs y).\n",
    "\n",
    "Thus, all classifiers and regressors in scikit-learn have specific implementations for the above functions. The differences among models (e.g., the different parameters they use) are treated either during construction or internally within these functions.\n",
    "\n",
    "For the particular case of SVMs for classification, the implementing class is <a href=\"http://scikit-learn.org/stable/modules/generated/sklearn.svm.SVC.html\">SVC</a>:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "SVC()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, by default scikit-learn builds an SVM with the RBF kernel. We know that such an SVM needs 2 parameters: C (regularizer between model complexity and classification errors) and gamma (kernel width). These are set to 1 and to 'auto' respectively, where 'auto' stands for 1/d (that is, 1 divided by the number of features).\n",
    "\n",
    "If you want to create an instance of an SVM with other values for these parameters, you just specify them (the rest of options for the constructor can be safely ignored for now):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SVC(C=5, gamma=0.01)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In practice it is difficult to know which values for C and gamma work best for a particular dataset. This is why it is common to try with a **grid** of values for both parameters, selecting the pair of values that result in the highest accuracy.\n",
    "\n",
    "The first step is selecting the parameter **ranges**. For this exercise, let us use these:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Cs = np.logspace(-2, 4, 7)\n",
    "gammas = np.logspace(-4, 4, 9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That is, C ranges from 0.01 to 10000, whereas gamma goes from 0.0001 to 10000, using powers of 10 as intermediate values.\n",
    "\n",
    "Recall that in order to avoid **overfitting**, we cannot use the test set for **tuning** these parameters. The following code keeps track of the accuracies obtained for the training set, selecting the best C and gamma:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Accuracies\n",
    "accs = np.zeros((len(Cs), len(gammas)))   # all accuracies (in matrix form)\n",
    "best_acc = 0.0   # best accuracy\n",
    "# For each C\n",
    "for i, C in enumerate(Cs):\n",
    "    # For each sigma\n",
    "    for j, gamma in enumerate(gammas):\n",
    "        # Create and train SVM\n",
    "        svm = SVC(C=C, gamma=gamma).fit(X_train, y_train)\n",
    "        # Keep track of accuracies and best params\n",
    "        acc = svm.score(X_train, y_train)\n",
    "        accs[i, j] = acc\n",
    "        if acc > best_acc:\n",
    "            best_C = C\n",
    "            best_gamma = gamma\n",
    "            best_acc = acc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is illustrative to plot these accuracies in matrix form:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(accs, interpolation='nearest', cmap=plt.cm.hot)\n",
    "plt.xlabel('gamma')\n",
    "plt.ylabel('C')\n",
    "plt.colorbar()\n",
    "plt.xticks(np.arange(len(gammas)), gammas, rotation=45)\n",
    "plt.yticks(np.arange(len(Cs)), Cs)\n",
    "plt.title('Accuracies obtained')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The best parameters turn out to be:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Best C = \" + str(best_C))\n",
    "print(\"Best gamma = \" + str(best_gamma))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Which give a perfect accuracy on the training set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Best accuracy on train = \" + str(best_acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cross-validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Perfect accuracy may seem like great news. However, the performance on the test set is not so good:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model = SVC(C=best_C, gamma=best_gamma).fit(X_train, y_train)\n",
    "best_model.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a clear sign of overfitting on the training set. If you remember from the theory, ideally we should tune our model fitting it with the training set, but assessing performance on a separate set called **validation set**.\n",
    "\n",
    "However, usually there are not enough data to split them into separate training, validation and test sets. If that is the case, we can resort to **cross-validation**, which proceeds as follows:\n",
    "* The training set is partitioned into k subsets (called **folds**).\n",
    "* k different models are trained, using each of the k folds to assess performance and the remaining k-1 folds to fit each model.\n",
    "* The best model is that whose average performance on the k folds is best.\n",
    "\n",
    "Programming all this is laborious. Fortunately, the team behind scikit-learn have a whole <a href=\"http://scikit-learn.org/stable/modules/cross_validation.html\">cross-validation module</a>. In addition, the library also has a class specifically designed to perform grid search on the parameters you specify, which is called <a href=\"http://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html\">GridSearchCV</a>:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As the documentation states, in order to build an instance of *GridSearchCV*, you will need to specify two objects:\n",
    "* An **estimator**, that is, the internal model that is optimized (in this case an *SVC* instance).\n",
    "* A **parameter grid**, a dictionary where you specify the parameter names and the ranges to perform the search on (these ranges were defined above)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table align=\"left\">\n",
    " <tr><td width=\"80\"><img src=\"img/question.png\" style=\"width:auto;height:auto\"></td><td style=\"text-align:left\">\n",
    "     Use the code cell below to create a <i>GridSearchCV</i> object called <i>gs</i> with the SVM as estimator and the range for <i>C</i> and <i>gamma</i> we defined above.\n",
    " </td></tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "####### INSERT YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "GridSearchCV is implemented as well as a model, so its fit(X, y) method is the one that performs the grid search, doing cross-validation under the hood."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table align=\"left\">\n",
    " <tr><td width=\"80\"><img src=\"img/question.png\" style=\"width:auto;height:auto\"></td><td style=\"text-align:left\">\n",
    "     Call the fit method on your <i>gs</i> object to perform the grid search on the training set. Do not worry if it takes some time to complete. \n",
    " </td></tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table align=\"left\">\n",
    " <tr><td width=\"80\"><img src=\"img/exclamation.png\" style=\"width:auto;height:auto\"></td><td style=\"text-align:left\">\n",
    "     If you build your <i>GridSearchCV</i> object above with <i>verbose=1</i> you will see a trace of how the cross-validation process is going. For more details, set <i>verbose=2</i>. If your PC has several processors you can speed up the whole process by setting <i>n_jobs</i> accordingly.\n",
    " </td></tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "####### INSERT YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that the grid search is complete, let us recover the results obtained. To keep things as before, we would like to overwrite the following variables:\n",
    "* *accs*: accurary matrix for the different (C, gamma) pairs.\n",
    "* *best_acc*: best accuracy.\n",
    "* *best_C*: best value for C.\n",
    "* *best_gamma*: best value for gamma.\n",
    "* *best_model*: fitted model with the best C and the best gamma."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table align=\"left\">\n",
    " <tr><td width=\"80\"><img src=\"img/question.png\" style=\"width:auto;height:auto\"></td><td style=\"text-align:left\">\n",
    "     Recover these variables from the fitted <i>gs</i> object. Check the <i>attributes</i> section in the <a href=\"http://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html\">documentation</a>.\n",
    " </td></tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table align=\"left\">\n",
    " <tr><td width=\"80\"><img src=\"img/exclamation.png\" style=\"width:auto;height:auto\"></td><td style=\"text-align:left\">\n",
    "     You do not need to retrain <i>best_model</i> from <i>best_C</i> and <i>best_gamma</i>, it is available directly in the <i>best_estimator_</i> attribute (notice the final underscore). Think also about how to get the accuracies in matrix form from <i>cv_results_</i>. Numpy's <a href=\"https://docs.scipy.org/doc/numpy/reference/generated/numpy.reshape.html\">reshape</a> function may come in handy.\n",
    " </td></tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "####### INSERT YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table align=\"left\">\n",
    " <tr><td width=\"80\"><img src=\"img/question.png\" style=\"width:auto;height:auto\"></td><td style=\"text-align:left\">\n",
    "Replot the accuracies just retrieved. In what way is the resulting figure different from the previous one?\n",
    " </td></tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "####### INSERT YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table align=\"left\">\n",
    " <tr><td width=\"80\"><img src=\"img/question.png\" style=\"width:auto;height:auto\"></td><td style=\"text-align:left\">\n",
    "Reprint the best parameters and their accuracy on training. Explain the changes you observe.\n",
    " </td></tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "####### INSERT YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table align=\"left\">\n",
    " <tr><td width=\"80\"><img src=\"img/question.png\" style=\"width:auto;height:auto\"></td><td style=\"text-align:left\">\n",
    "Recalculate the accuracy on test. Dit it improve?\n",
    " </td></tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "####### INSERT YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pipelining"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So far we have assumed that, once the data are loaded, they are ready to be fed to our model without any changes. If you inspect the code above, there is no change in the dataset since creation till it is fed to the *fit(X, y)* method (note that splitting the patterns in validation folds does not change the patterns themselves).\n",
    "\n",
    "In practice this will rarely be the case. The data usually need some kind of previous transformations, such as making them all be uniform. This is what is known as **preprocessing**. Combining these transformations with other processes like cross-validation is tricky, as one must make sure that the transformations take place in each of the validation folds, so that the models are only fitted with properly transformed data.\n",
    "\n",
    "Once more, we are lucky that scikit-learn makes this task very easy. The class that encapsulates all this is called <a href=http://scikit-learn.org/stable/modules/generated/sklearn.pipeline.Pipeline.html>Pipeline</a>:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Conceptually, a pipeline is defined as a series of **transformers**, followed by one final step which must be an estimator. In the scikit-learn's jargon, this means that the class that takes the last step must implement the method *fit(X, y)*, whereas all the previous classes must implement a new method called *transform(X)* that makes some modification on the input data X. If those transformations are data-dependent and have to be learnt, they must also implement another method called *fit_transform(X, y)*.\n",
    "\n",
    "Scikit-learn hides this complexity by implementing Pipeline itself as a model as well. Thus, this class complies with the interface we saw in the previous section: \n",
    "* **fit(X, y)**: calls *fit_transform(X, y)* for all transformers, then calls *fit(X, y)* for the final estimator. This means that all transformations are successively learnt and applied before fitting the model with the properly transformed data.\n",
    "* **score(X, y)**: calls *transform(X)* for all transformers, then calls *score(X, y)* for the final estimator. The final model requires transformed data, and all the transformations have been learnt by *fit(X, y)*, so they can be now applied directly.\n",
    "\n",
    "As the <a href=\"http://scikit-learn.org/stable/modules/pipeline.html\">usage guide</a> describes, in order to build an instance of <a href=\"http://scikit-learn.org/stable/modules/generated/sklearn.pipeline.Pipeline.html\">Pipeline</a> you will need to specify a list of **steps**, that is, the transformers and the final estimator you wish to chain. For internal reference, each of these steps must be identified by a name as well as by its class, so the steps are passed as a list of (name, class) pairs. Consider the following example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Pipeline([('svm', SVC())])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Which builds a pipeline with just one estimator (the default SVM), named *svm*. \n",
    "\n",
    "Obviously, a pipeline makes sense only when there is more than a single step. To keep things simple, here we will add just one previous step before the SVM, which performs **normalization to zero mean and unit variance**. This is carried out by the <a href=\"http://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.StandardScaler.html\">StandardScaler</a> class:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This normalization makes all features be in the same scale, preventing that some feature may have a bigger influence than others just because of the different original scales. If you go back to the dataset's plot, you can see that the scale of the first coordinate is much larger than the second one ([-40, +60] range compared to [-0.3, +0.6]). If we applied an RBF SVM to the data as they are, when computing the kernel function the second coordinate would be negligible compared to the first one. \n",
    "\n",
    "However, the figure makes clear that both features are equally important for correct classification, so normalization should improve accuracies considerably. Let us see if this is indeed the case."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table align=\"left\">\n",
    " <tr><td width=\"80\"><img src=\"img/question.png\" style=\"width:auto;height:auto\"></td><td style=\"text-align:left\">\n",
    "     Build a pipeline called <i>pl</i> with a <i>StandardScaler</i> followed by an SVM. Name both steps as you please.\n",
    " </td></tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "####### INSERT YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Performing grid search is trivial now, since *pl* is a valid estimator for the *GridSearchCV* object."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table align=\"left\">\n",
    " <tr><td width=\"80\"><img src=\"img/question.png\" style=\"width:auto;height:auto\"></td><td style=\"text-align:left\">\n",
    "Rewrite the grid search for the pipeline. This should overwrite again the variables <i>accs</i>, <i>best_acc</i>, <i>best_C</i>, <i>best_gamma</i> and <i>best_model</i>.\n",
    " </td></tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table align=\"left\">\n",
    " <tr><td width=\"80\"><img src=\"img/exclamation.png\" style=\"width:auto;height:auto\"></td><td style=\"text-align:left\">\n",
    "     Since the pipeline steps are named, the way to access parameters now is <i>step__parameter</i> (notice the double underscore). For example, in the pipeline above, we would access the <i>C</i> parameter with <i>svm__C</i>.\n",
    " </td></tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "####### INSERT YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table align=\"left\">\n",
    " <tr><td width=\"80\"><img src=\"img/question.png\" style=\"width:auto;height:auto\"></td><td style=\"text-align:left\">\n",
    "Replot the new accuracies. Are they different from the ones without the scaler?\n",
    " </td></tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "####### INSERT YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table align=\"left\">\n",
    " <tr><td width=\"80\"><img src=\"img/question.png\" style=\"width:auto;height:auto\"></td><td style=\"text-align:left\">\n",
    "Reprint the best parameters and their accuracy both on training and test. How did they change? Are they better now?\n",
    " </td></tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "####### INSERT YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center>\n",
    "~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.<br>\n",
    "                          THIS IS THE END OF THE ASSIGNMENT<br>\n",
    "~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.<br>\n",
    "</center>"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
